#!/usr/bin/env python3
"""
oaSentinel Data Processing Executable
Professional data processing with strict error handling
"""

import sys
import os
import argparse
from pathlib import Path

# Add src to path for imports
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

try:
    from oasentinel.data.crowdhuman import CrowdHumanProcessor
except ImportError as e:
    print(f"FATAL ERROR: Cannot import oaSentinel modules: {e}")
    print("Ensure src/oasentinel package is properly installed")
    sys.exit(1)


def validate_raw_dataset(raw_dir: Path):
    """Validate raw CrowdHuman dataset - FAIL if invalid"""
    
    if not raw_dir.exists():
        print(f"FATAL ERROR: Raw dataset directory not found: {raw_dir}")
        print("Download CrowdHuman dataset first: bin/oas-download")
        sys.exit(1)
    
    # Check for required CrowdHuman files
    required_files = [
        'annotation_train.odgt',
        'annotation_val.odgt',
        'Images'
    ]
    
    missing_files = []
    for file_name in required_files:
        file_path = raw_dir / file_name
        if not file_path.exists():
            missing_files.append(file_name)
    
    if missing_files:
        print(f"FATAL ERROR: Missing required CrowdHuman files: {missing_files}")
        print(f"Expected structure in {raw_dir}:")
        print("  annotation_train.odgt")
        print("  annotation_val.odgt")
        print("  Images/")
        sys.exit(1)
    
    # Count images
    images_dir = raw_dir / "Images"
    image_files = list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png"))
    
    if len(image_files) == 0:
        print(f"FATAL ERROR: No images found in {images_dir}")
        sys.exit(1)
    
    print(f"âœ“ Raw dataset validated: {len(image_files)} images")


def validate_splits(splits_str: str) -> tuple:
    """Validate split ratios - FAIL if invalid"""
    
    try:
        splits = [float(x.strip()) for x in splits_str.split(',')]
    except ValueError:
        print(f"FATAL ERROR: Invalid splits format '{splits_str}'")
        print("Use format: 0.8,0.15,0.05")
        sys.exit(1)
    
    if len(splits) != 3:
        print(f"FATAL ERROR: Must specify exactly 3 splits (train,val,test), got {len(splits)}")
        sys.exit(1)
    
    if abs(sum(splits) - 1.0) > 0.001:
        print(f"FATAL ERROR: Splits must sum to 1.0, got {sum(splits)}")
        sys.exit(1)
    
    if any(s < 0 or s > 1 for s in splits):
        print(f"FATAL ERROR: All splits must be between 0 and 1")
        sys.exit(1)
    
    return tuple(splits)


def get_default_paths():
    """Get default CrowdHuman dataset paths (shared location)"""
    crowdhuman_root = Path.home() / "CrowdHuman"
    return {
        'input': crowdhuman_root / "raw",
        'output': crowdhuman_root / "processed"
    }


def main():
    defaults = get_default_paths()
    
    parser = argparse.ArgumentParser(
        description='oaSentinel Data Processing - Convert CrowdHuman to YOLO format',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
Examples:
  %(prog)s                                                    # Use default shared locations
  %(prog)s --input ~/CrowdHuman/raw --output ~/CrowdHuman/processed  # Explicit shared locations
  %(prog)s --input data/raw/crowdhuman --output data/processed       # Project-specific locations
  %(prog)s --splits 0.8,0.15,0.05                           # Custom split ratios

Default paths:
  Input:  {defaults['input']}
  Output: {defaults['output']}
        """
    )
    
    parser.add_argument('--input', type=Path, default=defaults['input'],
                       help=f'Input directory containing raw CrowdHuman dataset [default: {defaults["input"]}]')
    parser.add_argument('--output', type=Path, default=defaults['output'],
                       help=f'Output directory for processed YOLO format [default: {defaults["output"]}]')
    parser.add_argument('--splits', type=str, default='0.8,0.15,0.05',
                       help='Train/val/test split ratios (default: 0.8,0.15,0.05)')
    
    args = parser.parse_args()
    
    print("ðŸŽ¯ oaSentinel Professional Data Processing")
    print("=" * 50)
    
    # Strict validation - NO FALLBACKS
    validate_raw_dataset(args.input)
    splits = validate_splits(args.splits)
    
    print(f"âœ“ Configuration validated")
    print(f"  Input: {args.input}")
    print(f"  Output: {args.output}")
    print(f"  Splits: train={splits[0]:.1%}, val={splits[1]:.1%}, test={splits[2]:.1%}")
    if args.input == defaults['input'] and args.output == defaults['output']:
        print(f"  Using shared dataset location (recommended)")
    print()
    
    # Process dataset
    try:
        processor = CrowdHumanProcessor(args.input, args.output)
        
        print("Processing CrowdHuman dataset...")
        stats = processor.process(splits=splits)
        
        print("\nðŸŽ‰ Data processing completed successfully!")
        print(f"Processed images: {stats.get('total_images', 'N/A')}")
        print(f"Total annotations: {stats.get('total_annotations', 'N/A')}")
        print(f"Output directory: {args.output}")
        
        # Create dataset YAML in appropriate location
        if args.output == defaults['output']:
            # Using shared location - create config in shared location
            yaml_path = Path.home() / "CrowdHuman" / "configs" / "dataset.yaml"
            yaml_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(yaml_path, 'w') as f:
                f.write(f"""# CrowdHuman Dataset Configuration - Shared Location
# Generated by oaSentinel processing on {PROJECT_ROOT}

path: {args.output}  # Absolute path to processed dataset
train: images/train  # Train images (relative to 'path')
val: images/val     # Val images (relative to 'path') 
test: images/test   # Test images (relative to 'path')

# Classes for dual-class human detection
nc: 2  # number of classes
names:
  0: person  # Full body detection
  1: head    # Head detection

# Dataset information
dataset_name: "CrowdHuman"
raw_data_path: "{args.input}"
processed_data_path: "{args.output}"
""")
            print(f"Shared dataset configuration: {yaml_path}")
            
            # Also create/update project config to point to shared location
            project_yaml = PROJECT_ROOT / "configs" / "crowdhuman.yaml" 
            with open(project_yaml, 'w') as f:
                f.write(f"""# oaSentinel CrowdHuman Configuration - Points to shared dataset
path: {args.output}  # Absolute path to shared processed dataset
train: images/train
val: images/val 
test: images/test

nc: 2
names:
  0: person
  1: head
""")
            print(f"Project configuration updated: {project_yaml}")
        else:
            # Using project-specific location
            yaml_path = PROJECT_ROOT / "configs" / "crowdhuman.yaml"
            print(f"Project dataset configuration: {yaml_path}")
        
    except Exception as e:
        print(f"\nFATAL ERROR during processing: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
